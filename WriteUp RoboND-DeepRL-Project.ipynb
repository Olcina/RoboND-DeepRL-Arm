{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBOTICS ND - DeepRL - Project\n",
    "\n",
    "## Reward Functions: Explain the reward functions that you created.\n",
    "\n",
    "As proposed in task 5 for the project and smoothed average of the mean of the distance of the goal is used to reward the arm when getting close to the object.\n",
    " \n",
    "``` CPP\n",
    "\n",
    "const float distDelta  = lastGoalDistance - distGoal;\n",
    "const float alpha = 0.5f;\n",
    "float average_delta = 0.0f;\n",
    "\n",
    "// compute the smoothed moving average of the delta of the distance to the goal\n",
    "average_delta = (average_delta * alpha) + (distDelta * (1.0f - alpha));\n",
    "rewardHistory = average_delta * REWARD_WIN;\n",
    "newReward     = true;\n",
    "\n",
    "\n",
    "```\n",
    "The reward parameters are the following:\n",
    "\n",
    "- REWARD_WIN  1.0f\n",
    "- REWARD_LOSS -1.0f\n",
    "\n",
    "The REWARD_LOSS is applied when the object touches the ground, then the rewardHistory is set to REWARD_LOSS and the episode is ended. The variable checkGroundContact is defined as true when the lowest part of the gripper touch the ground.\n",
    "\n",
    "\n",
    "``` cpp\n",
    "const float groundContact = 0.00f;\n",
    "bool checkGroundContact = false;\n",
    "\t\t\n",
    "\t\tif (gripBBox.min.z <= groundContact) {\n",
    "\t\t\tcheckGroundContact = true;\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tif(checkGroundContact)\n",
    "\t\t{\n",
    "\t\t\t\t\t\t\n",
    "\t\t\tif(DEBUG){printf(\"GROUND CONTACT, EOE\\n\");}\n",
    "\n",
    "\t\t\trewardHistory = REWARD_LOSS;\n",
    "\t\t\tnewReward     = true;\n",
    "\t\t\tendEpisode    = true;\n",
    "\t\t}\n",
    "\n",
    "```\n",
    "\n",
    "On the contrary, for a REWARD_WIN is delivered when the arm collision with the object, the function to check the collision can be found below. When the condition is meet the function add the REWARD_WIN as an extra to the rewardHistory to reinforce the learning, the episode is ended to continue with the training.\n",
    "\n",
    "\n",
    "``` cpp\n",
    "if (strcmp(contacts->contact(i).collision1().c_str(), COLLISION_ITEM) == 0)\n",
    "\t\t{\n",
    "\t\t\trewardHistory = rewardHistory + REWARD_WIN;\n",
    "\t\t\tnewReward  = true;\n",
    "\t\t\tendEpisode = true;\n",
    "\t\t\treturn;\n",
    "\t\t}\n",
    "\n",
    "```\n",
    "\n",
    "The Function above, is valid to the first objective of this project. It has a variable named COLLISION_ITEM == \"tube::tube_link::tube_collision\", and that means in this case we are checking whenever the robot touch the object.\n",
    "\n",
    "For the second objective we check the collision between the arm (collision2) and the gripper, defined as COLLISION_POINT  \"arm::gripperbase::gripper_link\". The function in this case is as follows.\n",
    "\n",
    "``` cpp\n",
    "if (strcmp(contacts->contact(i).collision2().c_str(), COLLISION_POINT) == 0)\n",
    "\t\t{\n",
    "\t\t\trewardHistory = rewardHistory + REWARD_WIN;\n",
    "\t\t\tnewReward  = true;\n",
    "\t\t\tendEpisode = true;\n",
    "\t\t\treturn;\n",
    "\t\t}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Hyperparameters: Specify the hyperparameters that you selected for each objective, and explain the reasoning behind the selection.\n",
    "\n",
    "#### First Objective 90% accuracy for Arm Collision\n",
    "\n",
    "The first decision was to reduce the input width and height after some memory errors arose.\n",
    "\n",
    "The DQN python agent definition comes preload with two different optimizers, Adam(Adaptive Moment Estimation) and RMSprop. An optimizer should reduce the loss function as quickly as possible to obtain a good solution. RMSprop was chosen, with a learning rate of 0.1, while this learning rate could be too high and let to local minima or completely erroneous solutions, it behaves well for this problems.\n",
    "\n",
    "\n",
    "HYPERPARAMETERS\n",
    "- INPUT_WIDTH   64\n",
    "- INPUT_HEIGHT  64\n",
    "- OPTIMIZER \"RMSprop\"\n",
    "- LEARNING_RATE 0.1f\n",
    "- REPLAY_MEMORY 10000\n",
    "- BATCH_SIZE 8\n",
    "- USE_LSTM false\n",
    "- LSTM_SIZE 32\n",
    "- EPS_DECAY 80\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Second Objective 80% accuracy for Gripper Collision\n",
    "\n",
    "The hyperparameters chosen for the second objectives were the same as in the first objective.\n",
    "\n",
    "\n",
    "\n",
    "## Results: Explain the results obtained for both objectives. Include discussion on the DQN agent's performance for both objectives. Include watermarked images, or videos of your results.\n",
    "\n",
    "#### 90% accuracy for Arm Collision results\n",
    "\n",
    "A water-marked screenshoot of the arm accuracy after ~400 tries is shown below. \n",
    "\n",
    "Additionally the status of the project when the accuracy was reach can be found in this [Repository](https://github.com/Olcina/RoboND-DeepRL-Arm.git) commit \"a3f57c7\"\n",
    " \n",
    "![90 accuracy](images/90_accuracy.png)\n",
    "\n",
    "#### 80% accuracy for Arm Gripper results\n",
    "\n",
    "Similarly, the status of the project when the 80% accuracy was reach can be found in the same [Repository](https://github.com/Olcina/RoboND-DeepRL-Arm.git) same commit \"a3f57c7\"\n",
    "\n",
    "![80 accuracy](images/80_accuracy.png)\n",
    "\n",
    "Even when the solution for both objectives was reached with the same hyperparameter selection the performance in the second case is much worse than in the first one. 941 tries against 572, that suggest that the results can be dramatically improved. The reward function used was indeed very simple and did not take into account variables such as time or joint angle values. Even with that, this agent reaches good performances after several tries.\n",
    "\n",
    "There are different behaviors for the different objectives. For example, in the first objective, the arm generally overreached the object because of those where a valid solution, touching the object whit the arm. On the other hand, in the second objective, the agent learns quickly that a minimum angle between the two tubes of his arm is necessary to get positive rewards. Nevertheless, for both cases when the mentioned angle was two high the arm is not able to reach the objective and get stuck in a position that the average moving based regards cannot recover.\n",
    "\n",
    "## Future Work: Briefly discuss how you can improve your current results.\n",
    "\n",
    "As discussed before, the results can be dramatically improved. For example, we could define a reward function that counts the time passed between the begin and the end of the simulation and gives more reward when the arm reaches the objective quickly. This reward function could lead to better and smoother solutions. But maybe, in such case, a limit to joint forces should be applied, or even penalized.\n",
    "\n",
    "Another possible reward function could award the arm when touching with the arm but give more reward when touching whit the gripper, this could lead to avoiding more quick movements that lead to the arm touching the ground. In the same train of tough, penalize the arm when is close to the ground can avoid those situations two. In that case, an equilibrium between penalization for avoiding the ground and getting close to the object should be carefully chosen.\n",
    "\n",
    "![arm stucked](images/stuck.png)\n",
    "\n",
    "\n",
    "Lastly, the arm sometimes tends to bend himself too much, a penalization could be applied for delete this state (see image) but at the same time we would be limiting the arm's reach artificially and should not be used for a moving objective.\n",
    "\n",
    "Ref:\n",
    "Code for playing videos in jupyter notebook taken from: [stackoverflow](https://stackoverflow.com/questions/18019477/how-can-i-play-a-local-video-in-my-ipython-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\"  controls muted>\n",
       "  <source src=\"Videos/90_accuracy.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% accuracy video \n",
    "# Code from: https://stackoverflow.com/questions/18019477/how-can-i-play-a-local-video-in-my-ipython-notebook\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\"  controls muted>\n",
    "  <source src=\"Videos/90_accuracy.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\"  controls muted>\n",
       "  <source src=\"Videos/80_accuracy.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% accuracy video \n",
    "# Code from: https://stackoverflow.com/questions/18019477/how-can-i-play-a-local-video-in-my-ipython-notebook\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\"  controls muted>\n",
    "  <source src=\"Videos/80_accuracy.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
